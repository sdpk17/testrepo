import cv2
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense

# Load the dataset (images of hand gestures)
# Assuming you have a dataset of images in a folder called "dataset"
dataset = []
labels = []
for folder in os.listdir("dataset"):
    for file in os.listdir(os.path.join("dataset", folder)):
        img = cv2.imread(os.path.join("dataset", folder, file))
        img = cv2.resize(img, (224, 224))
        dataset.append(img)
        labels.append(folder)

# Convert the dataset to a numpy array
dataset = np.array(dataset)
labels = np.array(labels)

# Define the model
model = Sequential()
model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))
model.add(MaxPooling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu'))
model.add(MaxPooling2D((2, 2)))
model.add(Flatten())
model.add(Dense(64, activation='relu'))
model.add(Dense(len(np.unique(labels)), activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(dataset, np.array(pd.get_dummies(labels)), epochs=10)

# Define a function to recognize hand gestures
def recognize_gesture(img):
    img = cv2.resize(img, (224, 224))
    img = np.expand_dims(img, axis=0)
    prediction = model.predict(img)
    return np.argmax(prediction)

# Test the model
cap = cv2.VideoCapture(0)
while True:
    ret, frame = cap.read()
    if not ret:
        break
    gesture = recognize_gesture(frame)
    cv2.putText(frame, str(gesture), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)
    cv2.imshow('frame', frame)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break
cap.release()
cv2.destroyAllWindows()
